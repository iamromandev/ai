# A minimal note on AI created with LOVE.

### Gradient
> **"A gradient measures how much the output of a function changes if you change the inputs a little bit." — Lex Fridman (MIT)**

A gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning. In mathematical terms, a gradient is a partial derivative with respect to its inputs.

### Gradient Descent
Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression).

Applicable in other areas: 
1. control engineering (robotics, chemical, etc.)
2. computer games
3. mechanical engineering

#### Types of Gradient Descent:
1. Batch Gradient Descent
2. Stochastic Gradient Descent
3. Mini-batch Gradient Descent



